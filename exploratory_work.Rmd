Sentiment Analysis of #Obamacare Tweets
=======================================

In this homework, Aaron Palumbo and James Quacinella used twitter data to analyze the sentiment of tweets about Obamacare.

Required Packages
-----------------

```{r}
list.of.packages <- c("ggplot2", "ggmap", "rjson", "ROAuth", "twitteR", "streamR", "wordcloud", "tm", "plyr", "stringr", "RJSONIO")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(list.of.packages, new.packages)

# Maps
library(ggplot2)
library(ggmap)

# JSON
library(rjson)
library(RJSONIO)

# Twitter
library("ROAuth")
library("twitteR")
library("streamR")
library("wordcloud")

# Text tools
library("tm")
library("plyr")
library("stringr")
```


Getting Data From Twitter
-------------------------

On first run, we need to create an OAuth object. This generates a URL, which needs to be opened to generate a custom PIN number. Enter this PIN when asked by the handshake(...) function and you'll be authenticated. The code then saves this oauth object to file, so we can load it without reauthoirzing again.

Taken from: http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/

```{r eval=FALSE}
if(!file.exists("twitteR_credentials")){
  download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
  cred <- OAuthFactory$new(consumerKey='hJFNU2CUnrWP7wtYrOmdw',
                          consumerSecret='mYpRIVG92xbk58rFg1UDwGH4xKryWpgyz3obaqDVqM',
                          requestURL='http://api.twitter.com/oauth/request_token',
                          accessURL='http://api.twitter.com/oauth/access_token',
                          authURL='http://api.twitter.com/oauth/authorize')
  cred$handshake(cainfo="cacert.pem")
  # Enter in PIN
  registerTwitterOAuth(cred)
  save(list="cred", file="twitteR_credentials")
} else {
  print("Credentials exist")
}
```

However, we'll just load the credentials from file.

```{r}
# Check if obamacaretweets exists on disk, if not, create it.
if(!file.exists("obamacaretweets")){
  # Sign into twitter
  load("twitteR_credentials")
  registerTwitterOAuth(cred)

  # Get Tweets on ObamaCare and store their text
  obamacaretweets <- searchTwitter("#obamacare", n=1500, cainfo="cacert.pem")
  
  # Save to disk
  save(obamacaretweets, file="obamacaretweets")
} else {
  load("obamacaretweets")
}

# Check if obamacaretweets_text exists on disk, if not, create it.
if(!file.exists("obamacaretweets_text")){
  obamacaretweets_text <- sapply(obamacaretweets, function(x) x$getText())
  
  # Save to disk
  save(obamacaretweets_text, file="obamacaretweets_text")
} else {
  load("obamacaretweets_text")
}

```

Generating a Wordcloud
----------------------

First, we'll do some basic text formatting (convert to lowercase, remove puncuations, remove stop words and remove the word 'obamacare') and then we'll produce a word cloud from the tweets.

```{r}
# Create a corpus and turn it into a word cloud
obamacaretweets_corpus <- Corpus(VectorSource(obamacaretweets_text))
obamacaretweets_corpus <- tm_map(obamacaretweets_corpus, tolower)
obamacaretweets_corpus <- tm_map(obamacaretweets_corpus, removePunctuation)
obamacaretweets_corpus <- tm_map(obamacaretweets_corpus, function(x)removeWords(x,stopwords()))
obamacaretweets_corpus <- tm_map(obamacaretweets_corpus, function(x)removeWords(x, c("obamacare")))
wordcloud(obamacaretweets_corpus)
```

Analyzing Sentiment
-------------------

### Jeffrey Breen method:

Download Hu & Liu’s opinion lexicon:
site - http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
files - http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar (downloaded Nov. 8, 2013)

```{r}
pos.words <- scan('positive-words.txt', what='character', comment.char=';') 
neg.words <- scan('negative-words.txt', what='character', comment.char=';')
```

#### Score sentiment function per Jeffrey Breen

```{r}
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
	 # we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
	 # we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
	 scores <- laply(sentences, function(sentence, pos.words, neg.words) {
	 	
	 	 # clean up sentences with R's regex-driven global substitute, gsub():
	 	 sentence <- gsub('[[:punct:]]', '', sentence)
	 	 sentence <- gsub('[[:cntrl:]]', '', sentence)
	 	 sentence <- gsub('\\d+', '', sentence)
	 	 # and convert to lower case:
	 	 sentence <- tolower(sentence)
	 	 # split into words. str_split is in the stringr package
	 	 word.list <- str_split(sentence, '\\s+')
	 	 # sometimes a list() is one level of hierarchy too much
	 	 words <- unlist(word.list)
	 	 # compare our words to the dictionaries of positive & negative terms
	 	 pos.matches <- match(words, pos.words)
	 	 neg.matches <- match(words, neg.words)
	
	 	 # match() returns the position of the matched term or NA
	 	 # we just want a TRUE/FALSE:
	 	 pos.matches <- !is.na(pos.matches)
	 	 neg.matches <- !is.na(neg.matches)
	 	 # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
	 	 score <- sum(pos.matches) - sum(neg.matches)
	 	 return(score)
	 }, pos.words, neg.words, .progress=.progress )
	 scores.df <- data.frame(score=scores, text=sentences)
	 return(scores.df)
}
```

#### Example / Sanity check

```{r eval=FALSE}
sample = c("You're awesome and I love you",
"I hate and hate and hate. So angry. Die!",
"Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity.")

result <- score.sentiment(sample, pos.words, neg.words)

```

#### Score Obamacare Tweets

It's interesting to note that using this technique, all you need to do to get a really negative score is repeat yourself. The tweet with the most negative score (-14) is:  
  
> obamacare lied lied lied lied lied lied lied lied lied lied lied lied lied whew that wasnt hard now u dems try itall together nowobamacare lied lied lied lied lied lied lied lied lied lied lied lied lied whew that wasnt hard now u dems try itall together now  

Compare that to this one with a score of (-1):

> crush the marxists obama obamacare defundobamacare defundthegop defundtherinos makedclisten tcot tlot teaparty gop rinos rnc  


```{r}
result <- score.sentiment(obamacaretweets_text, pos.words, neg.words)

# quick visualization of result
hist(result$score)
```

Using ViralHeat API
-------------------

Using the API key of 6LUTxvF9YYYjKWEZZ7X, we'll use ViralHEat's API to get an alternative way of doing sentiment analysis. The getSentiment() function handles the queries we send to the API and splits the positive and negative statements out of the JSON reply and returns them in a list.

Original code: http://thinktostart.wordpress.com/2013/09/02/sentiment-analysis-on-twitter-with-viralheat-api/


```{r}
getSentiment <- function (text, key){
  library(RCurl);
  library(RJSONIO);
  
  text <- URLencode(text, reserved=TRUE);
  
  #save all the spaces, then get rid of the weird characters that break the API, then convert back the URL-encoded spaces.
  text <- str_replace_all(text, "%20", " ");
  text <- str_replace_all(text, "%\\d\\d", "");
  text <- str_replace_all(text, " ", "%20");
  
  if (str_length(text) > 360){
    text <- substr(text, 0, 359);
  }
  
  data <- getURL(paste("https://www.viralheat.com/api/sentiment/review.json?api_key=", key, "&text=",text, sep=""), cainfo="cacert.pem")
  
  js <- fromJSON(data, asText=TRUE);
  
  # get mood probability
  score <- js$prob
  
  # positive, negative or neutral?
  if (js$mood != "positive"){
    if (js$mood == "negative") {
      score <- -1 * score
    } else {
      # neutral
      score <- 0
    }
  }

return(list(mood=js$mood, score=score))
}
```

### The clean.text() function

We need this function because of the problems occurring when the tweets contain some certain characters and to remove characters like “@” and “RT”.

```{r}
clean.text <- function(some_txt){
  some_txt <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
  some_txt <- gsub("@\\w+", "", some_txt)
  some_txt <- gsub("[[:punct:]]", "", some_txt)
  some_txt <- gsub("[[:digit:]]", "", some_txt)
  some_txt <- gsub("http\\w+", "", some_txt)
  some_txt <- gsub("[ \t]{2,}", "", some_txt)
  some_txt <- gsub("^\\s+|\\s+$", "", some_txt)
  some_txt <- gsub("([^a-zA-Z0-9 ])", "", some_txt)   # this takes whatever is left and
                                                      # removes anything that isn't a 
                                                      # letter, number, or space
  
  # define "tolower error handling" function
  try.tolower <- function(x){
    y <- NA
    try_error <- tryCatch(tolower(x), error=function(e) e)
    if (!inherits(try_error, "error"))
    y <- tolower(x)
    return(y)
  }
    
  some_txt <- sapply(some_txt, try.tolower)
  some_txt <- some_txt[some_txt != ""]
  names(some_txt) <- NULL
  return(some_txt)
}
```

We already have the tweets from above, now we need to clean them

```{r}
tweet_clean <- clean.text(obamacaretweets_text)
mcnum <- length(tweet_clean)
tweet_df <- data.frame(text=tweet_clean, sentiment=rep("", mcnum), score=1:mcnum, stringsAsFactors=FALSE)
```

### Do the ViralHeat Analysis

issue: slow - about 10 minutes to score 300 items
issue: look into set dependency on this chunk. Needs to run if obamatweets_text changes

```{r}
if (file.exists("tweet_df")){
  load("tweet_df")
} else {
  sentiment <- rep(0, mcnum) # what is this used for?
  start.time <- Sys.time()
  for (i in 1:mcnum){
    tmp <- getSentiment(tweet_clean[i], "6LUTxvF9YYYjKWEZZ7X")
    tweet_df$sentiment[i] <- tmp$mood
    tweet_df$score[i] <- tmp$score
    if (i%%20 == 0){
      func.time <- Sys.time()
      print(c(i, func.time-start.time))
    }
  }
  save(tweet_df, file="tweet_df")
}  
```

### Results

```{r}
hist(tweet_df$score)
```

### Compare results from the two techniques

Merge the two data sets:

One of the issues in comparing these two methods of sentiment analysis is that the scale used to rank the sentiment is entirely different. In the Jeffrey Breen method we simply count positive and negative words, in the ViralHeat API we get a positive or negative percentage that expresses how confident we are in the sentiment. In order to put plot these on the same scale, I modifed the Breen approach slightly; instead of just reporting the the sum of the positive and negative words, I divide that sum by the total number of words in the tweet. In that way, I make sure that the two methods are at least on the same scale, even if their magnitudes mean something slightly different. In the end, the graphic below is useful to see how strongly each method rated the tweet, but the exact delta in amplitudes is not really significant.


```{r}

n <- nrow(tweet_df)
tweet_df$jbscore <- result$score

# Order the data by score from ViralHeat
tweet_df <- tweet_df[order(tweet_df$score),]
vh <- tweet_df$score

# Scale jbscore by dividing score by total words from tweet
# This puts the two algorithms on rought the same scale
count_words <- function(text){
  sapply(gregexpr("\\W+", text), length) + 1
}
word.count <- lapply(tweet_df$text, count_words)
jb <- tweet_df$jbscore / as.numeric(word.count)

```

Create a plot to visualize the differences:

```{r fig.height=7, fig.width=12}
par(mar=c(5.1, 4.1, 4.1, 2.1) * 1) # Use this to adjust axes default is par(mar = c(5.1, 4.1, 4,1, 2,1))

plot(1:10, type="n", xlim=c(0,n), ylim=c(-1,1), 
     xlab="Tweet Index", ylab="Magnitude of Sentiment",
     main="Comparison of Sentiment Rating Methods")

for(i in 1:n){
  color="green"
  if(vh[i] * jb[i] < 0){color="red"}
  if(jb[i] == 0){color="grey"}
  segments(i, vh[i], i, jb[i], col=color, lwd=3)
}
points(vh, pch=19)
points(jb, pch=19, col="blue")

legend("topleft", inset=0.01, c("Scores from ViralHeat", "Scores from Breen Method"), pch=19, col=c("black", "blue"))
legend("bottomright", inset=0.01, 
       c("Sentiment rating agrees",
         "Sentiment rating disagrees",
         "Breen returned score of 0"), 
       lty = c("solid", "solid", "solid"), 
       lwd = 3,
       col = c("green", "red", "grey"))
abline(h=0)

```


Looking at the above figure we can see quite a bit of discrepency between the two methods. Since the data is sorted by the ViralHeat score, we would expect to see green on the outside of the graph and red and grey in the middle. There is maybe some hint of that with the grey, but the red does not seem to folow that pattern. From this, we can conclude that there is not good agreement between the two methods.

Conclusion
----------

From our results, sentiment analysis appears to be a difficult task that is subject to a large error. It would be interesting to also analyze results form Alchemy API to see if that had better corrleation to one of the methods. Also, it would be interesting to manually rate some number of these tweets to determine an overall accuracy. Another option that was considered was to plot the tweets on a map to see if patterns in location could be discerned from the data. Some progress was made, as can be seen in the appendix, but several issues need to be resolved before this can be used.


Appendix
--------

Plotting on maps
----------------

Lots of good info here: https://dl.dropboxusercontent.com/u/24648660/ggmap%20useR%202012.pdf

### Partial example from above source:

```{r eval=FALSE}
houston <- get_map('houston', zoom = 14)
HoustonMap <- ggmap(houston, extent = 'device', legend = 'topleft')
HoustonMap
```

### Map of USA

```{r eval=FALSE}
usa <- get_map('usa', zoom = 4)
usaMap <- ggmap(usa, extent = 'device', legend = 'topleft')
usaMap
```

### Map of NYC Tweets

```{r}
if (file.exists("filtered_nyc_tweets")){
  load("filtered_nyc_tweets")
} else {
  
  # Sign into twitter
  load("twitteR_credentials")
  registerTwitterOAuth(cred)# Bounding box for NYC
  
  # Bounding box for NYC
  min_lon_nyc <- -74
  min_lat_nyc <- 40
  max_lon_nyc <- -73
  max_lat_nyc <- 41
  
  # Stream tweets that are either obamacare or from NYC
  filterStream(file="obama_nyc.json", track="#obamacare", locations=c(min_lon_nyc, min_lat_nyc, max_lon_nyc, max_lat_nyc), timeout=60, oauth=cred)
  obama_tweet_stream <- parseTweets("obama_nyc.json")
  
  # Convert long and lat columns to decimals
  obama_tweet_stream$lon <- as.numeric(obama_tweet_stream$lon)
  obama_tweet_stream$lat <- as.numeric(obama_tweet_stream$lat)
  
  # Filter tweets that have geo information
  filtered_nyc_tweets <- obama_tweet_stream[!is.na(obama_tweet_stream$lon) & 
                                             !is.na(obama_tweet_stream$lat) &
                                             obama_tweet_stream$lon >= min_lon_nyc &
                                             obama_tweet_stream$lon <= max_lon_nyc & 
                                             obama_tweet_stream$lat >= min_lat_nyc &
                                             obama_tweet_stream$lat <= max_lat_nyc, ]
  
# Commented out this section, as there is no point in doing the sentiment analysis
# right now. I was going to use this info to color and size the points on the map
#   num_tweets <- nrow(filtered_nyc_tweets)
#   filtered_nyc_tweets$sentiment <- rep(0, num_tweets)
#   filtered_nyc_tweets$score <- rep(0, num_tweets)
#   
#   start.time <- Sys.time()
#   for (i in 1:num_tweets){
#     tmp <- getSentiment(filtered_nyc_tweets[i, 'text'], "6LUTxvF9YYYjKWEZZ7X")
#     filtered_nyc_tweets$sentiment[i] <- tmp$mood
#     filtered_nyc_tweets$score[i] <- tmp$score
#     if (i %% 20 == 0){
#       print(c(i, Sys.time() - start.time))
#     }
#   }
  
  save(filtered_nyc_tweets, file="filtered_nyc_tweets")
}

# Using the data itself, we can filter out the tweets that were far out
lon_stats = boxplot(filtered_nyc_tweets$lon, plot=FALSE)$stats
min_lon_data = lon_stats[2]
max_lon_data = lon_stats[4]
lat_stats = boxplot(filtered_nyc_tweets$lat, plot=FALSE)$stats
min_lat_data = lat_stats[2]
max_lat_data = lat_stats[4]

# Generate a map of NYC with the tweets plotted
nycMap <- qmap(location = c(min_lon_data, min_lat_data, max_lon_data, max_lat_data))
nycMap + geom_point(aes(x=lon, y=lat), data=filtered_nyc_tweets)
```


### Radius search example

This has some issues

```{r eval=FALSE}
twitterMap <- function(searchtext,locations,radius){
  require(ggplot2)
  require(maps)
  require(twitteR)
  #radius from randomly chosen location
  radius=radius
  lat<-runif(n=locations,min=24.446667, max=49.384472)
  long<-runif(n=locations,min=-124.733056, max=-66.949778)
  #generate data fram with random longitude, latitude and chosen radius
  coordinates<-as.data.frame(cbind(lat,long,radius))
  coordinates$lat<-lat
  coordinates$long<-long
  #create a string of the lat, long, and radius for entry into searchTwitter()
  for(i in 1:length(coordinates$lat)){
    coordinates$search.twitter.entry[i]<-toString(c(coordinates$lat[i],
    coordinates$long[i],radius))
  }
  # take out spaces in the string
  coordinates$search.twitter.entry<-gsub(" ","", coordinates$search.twitter.entry ,
  fixed=TRUE)
  
  #Search twitter at each location, check how many tweets and put into dataframe
  for(i in 1:length(coordinates$lat)){
    coordinates$number.of.tweets[i]<-
     length(searchTwitter(searchString=searchtext,n=1000,geocode=coordinates$search.twitter.entry[i]))
  }
  #making the US map
  all_states <- map_data("state")
  #plot all points on the map
  p <- ggplot()
  p <- p + geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour="grey",     fill=NA )
  
  p<-p + geom_point( data=coordinates, aes(x=long, y=lat,color=number.of.tweets
                                       )) + scale_size(name="# of tweets")
  p
}
# Example
searchTwitter("obamacare",15,"10mi")
```
